{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2Dfp3gVnobRS"
      ],
      "authorship_tag": "ABX9TyOCfZu7ie0DnlUCjpswlGKN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njones61/ce547/blob/main/docs/unit3/12_flopy/biglake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Big Lake Model\n",
        "\n",
        "In this workbook will import and explore an existing model that was genereated in GMS."
      ],
      "metadata": {
        "id": "bQXa6f3cnpMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install FloPy and MODFLOW executables"
      ],
      "metadata": {
        "id": "PNpLjc70n2b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is only necessary to get rid of deprecation warnings.\n",
        "# Try skipping this first. If you see a ton of deprecation warnings\n",
        "# then come back and run it.\n",
        "\n",
        "!pip install --upgrade jupyter_client"
      ],
      "metadata": {
        "id": "3yvRQisSoC4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyKLk_u3nCgI"
      },
      "outputs": [],
      "source": [
        "# Install FloPy and MODFLOW executables\n",
        "!pip install -q flopy\n",
        "!get-modflow :flopy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import flopy\n",
        "import flopy.plot as fp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Verify installation\n",
        "print(f\"FloPy version: {flopy.__version__}\")\n",
        "\n",
        "# Check MODFLOW executables\n",
        "import flopy.utils\n",
        "exe_name = 'mf2000'\n",
        "exe_path = flopy.which(exe_name)\n",
        "if exe_path:\n",
        "    print(f\"MODFLOW 2000 executable found at: {exe_path}\")\n",
        "else:\n",
        "    print(\"Warning: MODFLOW executable not found!\")\n"
      ],
      "metadata": {
        "id": "3WWy4iblswSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define helper functions"
      ],
      "metadata": {
        "id": "2Dfp3gVnobRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_inputs(model):\n",
        "    \"\"\"\n",
        "    Plot model inputs including grid and boundary conditions.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : flopy.modflow.Modflow\n",
        "        The MODFLOW model object\n",
        "    \"\"\"\n",
        "    # Create a figure with specified size\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Create the plotmapview object\n",
        "    pmap = fp.PlotMapView(model=model, layer=0)\n",
        "\n",
        "    # Plot the grid\n",
        "    pmap.plot_grid(lw=0.5, color=\"0.5\")\n",
        "    pmap.plot_inactive(color_noflow='gray')\n",
        "\n",
        "    # Plot boundary conditions\n",
        "    # Specified head cells (ibound = -1)\n",
        "    ibound = model.bas6.ibound.array[0]\n",
        "    if np.any(ibound == -1):\n",
        "        pmap.plot_ibound(color_ch='blue', alpha=0.3)\n",
        "\n",
        "    # Plot wells\n",
        "    if hasattr(model, 'wel') and model.wel is not None:\n",
        "        wel_data = model.wel.stress_period_data[0]\n",
        "        for well in wel_data:\n",
        "            k, i, j = int(well[0]), int(well[1]), int(well[2])\n",
        "            # Get cell center coordinates\n",
        "            x = model.modelgrid.xcellcenters[i, j]\n",
        "            y = model.modelgrid.ycellcenters[i, j]\n",
        "            plt.plot(x, y, 'ro', markersize=10, label='Wells' if well is wel_data[0] else '')\n",
        "\n",
        "    # Plot rivers\n",
        "    if hasattr(model, 'riv') and model.riv is not None:\n",
        "        riv_data = model.riv.stress_period_data[0]\n",
        "        for idx, riv in enumerate(riv_data):\n",
        "            k, i, j = int(riv[0]), int(riv[1]), int(riv[2])\n",
        "            x = model.modelgrid.xcellcenters[i, j]\n",
        "            y = model.modelgrid.ycellcenters[i, j]\n",
        "            plt.plot(x, y, 'cs', markersize=6, alpha=0.6, label='River' if idx == 0 else '')\n",
        "\n",
        "    # Plot GHB (general head boundaries)\n",
        "    if hasattr(model, 'ghb') and model.ghb is not None:\n",
        "        ghb_data = model.ghb.stress_period_data[0]\n",
        "        for idx, ghb in enumerate(ghb_data):\n",
        "            k, i, j = int(ghb[0]), int(ghb[1]), int(ghb[2])\n",
        "            x = model.modelgrid.xcellcenters[i, j]\n",
        "            y = model.modelgrid.ycellcenters[i, j]\n",
        "            plt.plot(x, y, 'g^', markersize=5, alpha=0.5, label='GHB' if idx == 0 else '')\n",
        "\n",
        "    # Create custom legend entry for specified heads\n",
        "    if np.any(ibound == -1):\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [Patch(facecolor='blue', alpha=0.3, label='Specified Head')]\n",
        "\n",
        "        # Add other BC elements\n",
        "        if hasattr(model, 'wel') and model.wel is not None:\n",
        "            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w',\n",
        "                                             markerfacecolor='r', markersize=10, label='Wells'))\n",
        "        if hasattr(model, 'riv') and model.riv is not None:\n",
        "            legend_elements.append(plt.Line2D([0], [0], marker='s', color='w',\n",
        "                                             markerfacecolor='c', markersize=6, label='River'))\n",
        "        if hasattr(model, 'ghb') and model.ghb is not None:\n",
        "            legend_elements.append(plt.Line2D([0], [0], marker='^', color='w',\n",
        "                                             markerfacecolor='g', markersize=5, label='GHB'))\n",
        "\n",
        "        plt.legend(handles=legend_elements, loc='best', fontsize=10)\n",
        "\n",
        "    # Add title and labels\n",
        "    plt.title(f'Model Inputs and Boundary Conditions - {model.name}')\n",
        "    plt.xlabel('X-coordinate [m]')\n",
        "    plt.ylabel('Y-coordinate [m]')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_solution(model, head, pathlines=None, max_time=None):\n",
        "    \"\"\"\n",
        "    Plot head distribution with contours and optionally pathlines.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : flopy.modflow.Modflow\n",
        "        The MODFLOW model object\n",
        "    head : numpy array\n",
        "        Head array with shape (nlay, nrow, ncol)\n",
        "    pathlines : list of numpy recarrays or None, optional\n",
        "        Pathline data from PathlineFile.get_alldata()\n",
        "    max_time : float or None, optional\n",
        "        Maximum time (in days) to display pathlines. If None, show all.\n",
        "    \"\"\"\n",
        "    # Create a figure with specified size (width, height in inches)\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Create the plotmapview object\n",
        "    pmap = fp.PlotMapView(model=model)\n",
        "\n",
        "    # Plot the heads\n",
        "    im = pmap.plot_array(head, cmap='viridis')\n",
        "    pmap.plot_inactive()\n",
        "    pmap.plot_ibound()\n",
        "\n",
        "    # Plot the grid lines\n",
        "    pmap.plot_grid(lw=0.5, color=\"0.5\")\n",
        "\n",
        "    # Add contours\n",
        "    interval = 2.0\n",
        "    levels = np.arange(np.floor(head.min()), np.ceil(head.max()) + interval, interval)\n",
        "    cs = pmap.contour_array(head, levels=levels, colors='black', linewidths=1.5)\n",
        "    plt.clabel(cs, fmt='%1.1f')\n",
        "\n",
        "    # Plot pathlines if provided, colored by particle group\n",
        "    if pathlines is not None:\n",
        "        # Filter pathlines by max_time if specified\n",
        "        if max_time is not None:\n",
        "            filtered_pathlines = []\n",
        "            for pline in pathlines:\n",
        "                # Filter points where time <= max_time\n",
        "                mask = pline['time'] <= max_time\n",
        "                if np.any(mask):\n",
        "                    filtered_pathlines.append(pline[mask])\n",
        "            pathlines = filtered_pathlines\n",
        "\n",
        "        # Group pathlines by particle group (each well creates a group)\n",
        "        from collections import defaultdict\n",
        "        pathlines_by_group = defaultdict(list)\n",
        "\n",
        "        for pline in pathlines:\n",
        "            # Get particle group number (should be in the data)\n",
        "            # If particlegroup field exists, use it; otherwise infer from particleid\n",
        "            if 'particlegroup' in pline.dtype.names:\n",
        "                group_id = pline['particlegroup'][0]\n",
        "            else:\n",
        "                # Infer group from particle ID (96 particles per well in this case)\n",
        "                particle_id = pline['particleid'][0]\n",
        "                group_id = (particle_id - 1) // 96  # 0, 1, or 2\n",
        "            pathlines_by_group[group_id].append(pline)\n",
        "\n",
        "        # Define colors for different wells\n",
        "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']\n",
        "        well_names = ['Well 1 (L1,R28,C65)', 'Well 2 (L1,R35,C34)', 'Well 3 (L1,R68,C88)']\n",
        "\n",
        "        # Plot each group with a different color\n",
        "        for group_id in sorted(pathlines_by_group.keys()):\n",
        "            plines = pathlines_by_group[group_id]\n",
        "            color = colors[group_id % len(colors)]\n",
        "            label = well_names[group_id] if group_id < len(well_names) else f'Well {group_id+1}'\n",
        "            pmap.plot_pathline(plines, colors=color, linewidths=1.0, alpha=0.7, label=label)\n",
        "\n",
        "        # Add legend\n",
        "        plt.legend(loc='best', fontsize=8)\n",
        "\n",
        "    # Add a color bar\n",
        "    plt.colorbar(im, label='Head [m]', shrink=0.8)\n",
        "\n",
        "    # Add title and labels\n",
        "    title = f'Head Distribution - {model.name}'\n",
        "    if pathlines is not None:\n",
        "        title += ' with Pathlines'\n",
        "    plt.title(title)\n",
        "    plt.xlabel('X-coordinate [m]')\n",
        "    plt.ylabel('Y-coordinate [m]')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "import re\n",
        "\n",
        "def fix_modflow_file_formatting(file_path):\n",
        "    \"\"\"\n",
        "    Fix concatenated numbers in MODFLOW boundary condition files (GHB, RIV).\n",
        "    FloPy sometimes writes numbers without sufficient spacing, causing concatenation.\n",
        "    This function splits concatenated numbers and truncates the right number to maintain alignment.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the MODFLOW file to fix\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    fixed_lines = []\n",
        "    for line in lines:\n",
        "        # Helper function to truncate decimal part by n digits\n",
        "        def truncate_decimal(decimal_str, n_digits):\n",
        "            \"\"\"Truncate the decimal part (after '0.') by n_digits from the right\"\"\"\n",
        "            if n_digits <= 0:\n",
        "                return decimal_str\n",
        "            # decimal_str is like \"0.00616674\"\n",
        "            if len(decimal_str) <= 2 + n_digits:  # \"0.\" + at least n_digits\n",
        "                # If too short, just return \"0.\" or minimal value\n",
        "                if len(decimal_str) <= 2:\n",
        "                    return \"0.\"\n",
        "                return decimal_str[:len(decimal_str) - n_digits]\n",
        "            return decimal_str[:-n_digits]\n",
        "\n",
        "        # 1. Decimal number followed by \"0.\" -> \"117.6050.00616674\" -> \"117.605 0.0061667\" (drop 1 digit)\n",
        "        def fix_decimal_concatenation(match):\n",
        "            left_part = match.group(1)\n",
        "            right_part = match.group(2)  # \"0.00616674\"\n",
        "            # We're adding 1 space, so drop 1 digit from right_part\n",
        "            truncated_right = truncate_decimal(right_part, 1)\n",
        "            return f'{left_part} {truncated_right}'\n",
        "\n",
        "        fixed_line = re.sub(r'(\\d+\\.\\d+)(0\\.\\d+)', fix_decimal_concatenation, line)\n",
        "\n",
        "        # 2. Integer followed by \"0.\" -> \"1200.003\" -> \"120 0.00\" (drop 1 digit)\n",
        "        #    But avoid matching valid decimals like \"120.0\" (which should not be changed)\n",
        "        def fix_integer_concatenation(match):\n",
        "            start_pos = match.start()\n",
        "            digits = match.group(1)\n",
        "            zero_decimal = match.group(2)\n",
        "\n",
        "            # Check if digits are part of an existing decimal number\n",
        "            if start_pos > 0:\n",
        "                # Check if immediately before is a digit, and before that might be a '.'\n",
        "                if start_pos >= 2:\n",
        "                    two_before = line[start_pos-2:start_pos]\n",
        "                    if re.match(r'\\d\\.', two_before):\n",
        "                        return match.group(0)  # Part of decimal like 'X.120.0'\n",
        "                # Check if before is a digit\n",
        "                if line[start_pos-1].isdigit():\n",
        "                    # Could be part of decimal - check further back\n",
        "                    lookback = max(0, start_pos-5)\n",
        "                    preceding = line[lookback:start_pos]\n",
        "                    if '.' in preceding:\n",
        "                        # There's a decimal point before, might be part of decimal number\n",
        "                        if re.search(r'\\d\\.\\d*$', preceding):\n",
        "                            return match.group(0)  # Don't change - part of decimal\n",
        "\n",
        "            # Only split if there are 3+ digits (indicating likely concatenation)\n",
        "            # Valid decimals like '10.0', '120.0' typically have 1-3 digits\n",
        "            if len(digits) >= 4:\n",
        "                # 4+ digits almost certainly concatenated (like '1200.003')\n",
        "                # We're adding 1 space, so drop 1 digit from zero_decimal\n",
        "                truncated_decimal = truncate_decimal(zero_decimal, 1)\n",
        "                return f'{digits} {truncated_decimal}'\n",
        "            elif len(digits) >= 3:\n",
        "                # For 3 digits, be more careful - only split if at field start\n",
        "                # and not part of a decimal (already checked above)\n",
        "                if start_pos == 0 or (start_pos > 0 and line[start_pos-1].isspace()):\n",
        "                    truncated_decimal = truncate_decimal(zero_decimal, 1)\n",
        "                    return f'{digits} {truncated_decimal}'\n",
        "\n",
        "            return match.group(0)  # Don't change short numbers (1-2 digits)\n",
        "\n",
        "        fixed_line = re.sub(r'(\\d+)(0\\.\\d+)', fix_integer_concatenation, fixed_line)\n",
        "        fixed_lines.append(fixed_line)\n",
        "\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.writelines(fixed_lines)\n",
        "\n"
      ],
      "metadata": {
        "id": "YYC7XKo9n3dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Model Files\n",
        "\n",
        "Upload the file as a zip archive (biglake.zip)."
      ],
      "metadata": {
        "id": "q8sfp7eApA2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab.files as files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"Please upload your zip file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
        "\n",
        "    # Get the base name without extension to create the directory\n",
        "    base_name = os.path.splitext(fn)[0]\n",
        "    extract_path = os.path.join('.', base_name)\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    # Save the uploaded file temporarily\n",
        "    with open(fn, 'wb') as f:\n",
        "        f.write(uploaded[fn])\n",
        "\n",
        "    # Unzip the file into the new directory\n",
        "    if zipfile.is_zipfile(fn):\n",
        "        with zipfile.ZipFile(fn, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(f\"Successfully unzipped '{fn}' into '{extract_path}/'\")\n",
        "    else:\n",
        "        print(f\"Error: '{fn}' is not a valid zip file.\")\n",
        "\n",
        "    # Optionally, remove the uploaded zip file after extraction\n",
        "    os.remove(fn)\n"
      ],
      "metadata": {
        "id": "WtESp5F0pApZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2a - Upload, inspect, and run the model"
      ],
      "metadata": {
        "id": "DUv-Z9sXr8Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model (exclude observation packages that may have parsing issues)\n",
        "print(\"Loading MODFLOW model...\")\n",
        "ml = flopy.modflow.Modflow.load('biglake.mfn', model_ws=extract_path,\n",
        "                                 load_only=['dis', 'bas6', 'lpf', 'rch', 'wel', 'riv', 'ghb', 'pcg', 'oc'],\n",
        "                                 check=False, version='mf2k', exe_name='mf2000')\n",
        "\n",
        "# Change model workspace to new location (so original files aren't modified)\n",
        "\n",
        "# Create a workspace directory\n",
        "model_ws = './my_model'\n",
        "os.makedirs(model_ws, exist_ok=True)\n",
        "ml.change_model_ws(model_ws)\n",
        "\n",
        "# Print summary information\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model name: {ml.name}\")\n",
        "print(f\"Model packages: {', '.join(ml.get_package_list())}\")\n",
        "print(f\"Model dimensions: {ml.nlay} layers, {ml.nrow} rows, {ml.ncol} columns\")\n",
        "print(f\"Grid spacing (delr): {ml.dis.delr.array.min():.2f} to {ml.dis.delr.array.max():.2f} [m]\")\n",
        "print(f\"Grid spacing (delc): {ml.dis.delc.array.min():.2f} to {ml.dis.delc.array.max():.2f} [m]\")\n",
        "print(f\"Number of stress periods: {ml.nper}\")\n",
        "print(f\"Time units: {ml.dis.itmuni_dict[ml.dis.itmuni]} [d]\")\n",
        "# Length units: 0=undefined, 1=feet, 2=meters, 3=centimeters\n",
        "lenuni_names = {0: 'undefined', 1: 'feet', 2: 'meters', 3: 'centimeters'}\n",
        "lenuni_name = lenuni_names.get(ml.dis.lenuni, 'unknown')\n",
        "print(f\"Length units: {lenuni_name} [m]\")\n",
        "\n",
        "# Print boundary conditions info\n",
        "ibound = ml.bas6.ibound.array\n",
        "n_active = np.sum(ibound == 1)\n",
        "n_inactive = np.sum(ibound == 0)\n",
        "n_specified = np.sum(ibound == -1)\n",
        "print(f\"\\nBoundary conditions:\")\n",
        "print(f\"  Active cells: {n_active}\")\n",
        "print(f\"  Inactive cells: {n_inactive}\")\n",
        "print(f\"  Specified head cells: {n_specified}\")\n",
        "\n",
        "# Print initial head info (only for active cells)\n",
        "strt = ml.bas6.strt.array\n",
        "ibound = ml.bas6.ibound.array\n",
        "# Mask out inactive cells (ibound == 0)\n",
        "strt_active = strt[ibound != 0]\n",
        "print(f\"\\nInitial head (active cells only):\")\n",
        "print(f\"  Minimum: {strt_active.min():.2f} [m]\")\n",
        "print(f\"  Maximum: {strt_active.max():.2f} [m]\")\n",
        "print(f\"  Mean: {strt_active.mean():.2f} [m]\")\n",
        "\n",
        "# Print hydraulic conductivity info (only for active cells)\n",
        "if hasattr(ml, 'lpf') and ml.lpf is not None:\n",
        "    hk = ml.lpf.hk.array\n",
        "    # Mask out inactive cells (ibound == 0)\n",
        "    hk_active = hk[ibound != 0]\n",
        "    print(f\"\\nHydraulic conductivity (HK, active cells only):\")\n",
        "    print(f\"  Minimum: {hk_active.min():.6e} [m/d]\")\n",
        "    print(f\"  Maximum: {hk_active.max():.6e} [m/d]\")\n",
        "    print(f\"  Mean: {hk_active.mean():.6e} [m/d]\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "FoP9kDJErxmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot model inputs and boundary conditions\n",
        "print(\"\\nPlotting model inputs and boundary conditions...\")\n",
        "plot_inputs(ml)"
      ],
      "metadata": {
        "id": "y_3z-kZAsMxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write model files to new workspace\n",
        "print(\"\\nWriting model files to new workspace...\")\n",
        "ml.write_input()\n",
        "\n",
        "# Fix both GHB and RIV files\n",
        "for fname in ['biglake.ghb', 'biglake.riv']:\n",
        "    fpath = os.path.join(model_ws, fname)\n",
        "    fix_modflow_file_formatting(fpath)"
      ],
      "metadata": {
        "id": "uslZ_wGethCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the MODFLOW model\n",
        "print(\"\\nRunning MODFLOW model...\")\n",
        "success, buff = ml.run_model(silent=True, report=True)\n",
        "\n",
        "if success:\n",
        "    print(\"Model ran successfully!\")\n",
        "else:\n",
        "    print(\"Model did not run successfully.\")\n",
        "    if isinstance(buff, list):\n",
        "        print('\\n'.join(buff))\n",
        "    else:\n",
        "        print(buff)"
      ],
      "metadata": {
        "id": "sOPBsFEvtzXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load head results\n",
        "print(\"\\nLoading head results from output file...\")\n",
        "head_file = os.path.join(model_ws, 'biglake.hed')\n",
        "hds = flopy.utils.binaryfile.HeadFile(head_file)\n",
        "\n",
        "# Get head for the last time step\n",
        "times = hds.get_times()\n",
        "head = hds.get_data(totim=times[-1])\n",
        "print(f\"Head data loaded for time: {times[-1]}\")\n",
        "\n",
        "# Print some head statistics (only for active cells)\n",
        "ibound = ml.bas6.ibound.array\n",
        "# Mask out inactive cells (ibound == 0)\n",
        "head_active = head[ibound != 0]\n",
        "print(f\"\\nHead statistics (active cells only):\")\n",
        "print(f\"  Minimum: {head_active.min():.2f} [m]\")\n",
        "print(f\"  Maximum: {head_active.max():.2f} [m]\")\n",
        "print(f\"  Mean: {head_active.mean():.2f} [m]\")\n"
      ],
      "metadata": {
        "id": "oFkfPxMut8JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the heads\n",
        "print(\"\\nPlotting head distribution...\")\n",
        "plot_solution(ml, head)\n"
      ],
      "metadata": {
        "id": "jl2yQ63Zt-bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2b - Particle Tracking Analysis"
      ],
      "metadata": {
        "id": "kPNAqarEuHOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flopy.modpath as mp\n",
        "\n",
        "# Set particle tracking time limit (days) for visualization\n",
        "tracking_time_days = 3650  # 10 years\n",
        "\n",
        "# Create MODPATH6 model\n",
        "mp_name = f'{ml.name}_mp'\n",
        "mp_model = mp.Modpath6(\n",
        "    modelname=mp_name,\n",
        "    modflowmodel=ml,\n",
        "    exe_name='mp6',\n",
        "    model_ws=model_ws\n",
        ")\n",
        "\n",
        "# Create MODPATH6 BAS package with porosity\n",
        "mpbas = mp.Modpath6Bas(mp_model, prsity=0.3)\n",
        "\n",
        "# Create simulation for backward tracking from wells\n",
        "print(f\"\\nTracking particles backward from wells (display limited to {tracking_time_days} days)...\")\n",
        "sim = mp_model.create_mpsim(\n",
        "    trackdir='backward',\n",
        "    simtype='pathline',\n",
        "    packages='WEL',\n",
        "    start_time=(0, 0, 0.0)\n",
        ")\n",
        "\n",
        "# Write and run MODPATH\n",
        "mp_model.write_input()\n",
        "mp_success, mp_buff = mp_model.run_model(silent=True, report=False)\n",
        "\n",
        "if mp_success:\n",
        "    # Load and plot pathline results\n",
        "    pathline_file = os.path.join(model_ws, f'{mp_name}.mppth')\n",
        "    pathlines = flopy.utils.PathlineFile(pathline_file)\n",
        "    pathline_data = pathlines.get_alldata()\n",
        "    print(f\"Loaded pathlines for {len(pathline_data)} particles\")\n",
        "\n",
        "    # Plot with time-filtered pathlines\n",
        "    plot_solution(ml, head, pathlines=pathline_data, max_time=tracking_time_days)\n",
        "else:\n",
        "    print(\"MODPATH did not run successfully\")\n",
        "    if mp_buff:\n",
        "        print('\\n'.join(str(x) for x in mp_buff) if isinstance(mp_buff, list) else str(mp_buff))"
      ],
      "metadata": {
        "id": "UaVoKPBWuG9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2c - Flow Budget Analysis"
      ],
      "metadata": {
        "id": "sOGk4uMGusxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load list file to extract budget summary\n",
        "list_file = os.path.join(model_ws, 'biglake.out')\n",
        "mfl = flopy.utils.Mf6ListBudget(list_file) if hasattr(flopy.utils, 'Mf6ListBudget') else None\n",
        "\n",
        "# Alternative: Load cell budget file directly\n",
        "cbc_file = os.path.join(model_ws, 'biglake.ccf')\n",
        "cbc = flopy.utils.CellBudgetFile(cbc_file)\n",
        "\n",
        "# Get budget for last time step\n",
        "times = cbc.get_times()\n",
        "print(f\"\\nFlow budget at time = {times[-1]} days:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Get budget records and compute totals\n",
        "records = cbc.get_unique_record_names()\n",
        "total_in = 0.0\n",
        "total_out = 0.0\n",
        "\n",
        "# Extract flow rates for each component\n",
        "budget_items = []\n",
        "for record in records:\n",
        "    record_name = record.decode().strip()\n",
        "    # Skip internal flow components\n",
        "    if 'FLOW' in record_name and 'FACE' in record_name:\n",
        "        continue\n",
        "\n",
        "    # Get data and sum flows\n",
        "    data_list = cbc.get_data(text=record, totim=times[-1])\n",
        "    flow_rate = 0.0\n",
        "\n",
        "    for data in data_list:\n",
        "        if isinstance(data, list):\n",
        "            # Special case: RECHARGE returns [layer_array, flow_array]\n",
        "            if len(data) >= 2 and isinstance(data[1], np.ndarray):\n",
        "                flow_rate += data[1].sum()\n",
        "        elif isinstance(data, np.ndarray):\n",
        "            if data.dtype.names is not None and 'q' in data.dtype.names:\n",
        "                # Structured array with 'q' field\n",
        "                flow_rate += data['q'].sum()\n",
        "            else:\n",
        "                # Regular array\n",
        "                flow_rate += np.sum(data)\n",
        "\n",
        "    budget_items.append((record_name, flow_rate))\n",
        "\n",
        "# Print budget components\n",
        "for name, flow in budget_items:\n",
        "    if flow > 0:\n",
        "        print(f\"  {name:20s}  IN:  {flow:12.2f} m³/d\")\n",
        "        total_in += flow\n",
        "    elif flow < 0:\n",
        "        print(f\"  {name:20s}  OUT: {flow:12.2f} m³/d\")\n",
        "        total_out += flow\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"  {'Total IN':20s}       {total_in:12.2f} m³/d\")\n",
        "print(f\"  {'Total OUT':20s}       {total_out:12.2f} m³/d\")\n",
        "print(f\"  {'Net (IN - OUT)':20s}       {total_in + total_out:12.2f} m³/d\")\n",
        "if total_in > 0:\n",
        "    print(f\"  {'Percent Error':20s}       {100 * (total_in + total_out) / total_in:12.4f} %\")"
      ],
      "metadata": {
        "id": "wUKBHyfEu1Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2d - GHB Conductance Sensitivity Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "HcXZcSaJu4V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sensitivity analysis parameters\n",
        "min_cond_factor = 0.1\n",
        "max_cond_factor = 10.0\n",
        "n_increments = 50\n",
        "\n",
        "# Generate conductance multiplier values (log distribution for more detail at low values)\n",
        "cond_factors = np.linspace(min_cond_factor, max_cond_factor, n_increments)\n",
        "\n",
        "# Store results\n",
        "ghb_net_flows = []\n",
        "\n",
        "# Get original GHB conductance values\n",
        "original_ghb_data = ml.ghb.stress_period_data[0].copy()\n",
        "\n",
        "print(f\"\\nRunning {n_increments} model simulations...\")\n",
        "print(f\"Conductance factor range: {min_cond_factor} to {max_cond_factor}\")\n",
        "\n",
        "for i, cond_factor in enumerate(cond_factors):\n",
        "    # Modify GHB conductance\n",
        "    modified_ghb_data = original_ghb_data.copy()\n",
        "    modified_ghb_data['cond'] = original_ghb_data['cond'] * cond_factor\n",
        "\n",
        "    # Update model with modified conductance\n",
        "    ml.ghb.stress_period_data[0] = modified_ghb_data\n",
        "\n",
        "    # Write only the GHB file\n",
        "    ml.ghb.write_file()\n",
        "\n",
        "    # Fix GHB file formatting (FloPy uses insufficient field width for MODFLOW-2000)\n",
        "    ghb_file = os.path.join(model_ws, 'biglake.ghb')\n",
        "    fix_modflow_file_formatting(ghb_file)\n",
        "\n",
        "\n",
        "    # Delete old output files to ensure fresh results\n",
        "    cbc_file = os.path.join(model_ws, 'biglake.ccf')\n",
        "    hed_file = os.path.join(model_ws, 'biglake.hed')\n",
        "    if os.path.exists(cbc_file):\n",
        "        os.remove(cbc_file)\n",
        "    if os.path.exists(hed_file):\n",
        "        os.remove(hed_file)\n",
        "\n",
        "    success_run, buff = ml.run_model(silent=True, report=False)\n",
        "\n",
        "    if success_run:\n",
        "        # Load cell budget file (fresh each time to avoid caching)\n",
        "        cbc_file = os.path.join(model_ws, 'biglake.ccf')\n",
        "        cbc = flopy.utils.CellBudgetFile(cbc_file)\n",
        "        times = cbc.get_times()\n",
        "\n",
        "        # Get GHB flow (same method as budget analysis)\n",
        "        ghb_data_list = cbc.get_data(text='HEAD DEP BOUNDS', totim=times[-1])\n",
        "        ghb_flow = 0.0\n",
        "        for data in ghb_data_list:\n",
        "            if isinstance(data, np.ndarray) and data.dtype.names is not None and 'q' in data.dtype.names:\n",
        "                ghb_flow += data['q'].sum()\n",
        "\n",
        "        ghb_net_flows.append(ghb_flow)\n",
        "    else:\n",
        "        ghb_net_flows.append(np.nan)\n",
        "\n",
        "    # Progress indicator\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"  Completed {i + 1}/{n_increments} simulations (factor={cond_factor:.3f}, GHB flow={ghb_net_flows[-1]:.2f} m³/d)\")\n",
        "\n",
        "# Restore original GHB data\n",
        "ml.ghb.stress_period_data[0] = original_ghb_data\n",
        "ml.write_input()\n",
        "\n",
        "print(f\"\\nCompleted all {n_increments} simulations\")\n",
        "\n",
        "# Print summary statistics\n",
        "ghb_array = np.array(ghb_net_flows)\n",
        "print(f\"\\nGHB Flow Statistics:\")\n",
        "print(f\"  Min: {np.nanmin(ghb_array):.2f} m³/d\")\n",
        "print(f\"  Max: {np.nanmax(ghb_array):.2f} m³/d\")\n",
        "print(f\"  Mean: {np.nanmean(ghb_array):.2f} m³/d\")\n",
        "\n",
        "# Plot results\n",
        "print(\"\\nPlotting sensitivity analysis results...\")\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.plot(cond_factors, ghb_net_flows, 'b-', linewidth=2, label='GHB Flow')\n",
        "ax.axvline(x=1.0, color='r', linestyle='--', linewidth=2, label='Baseline (factor=1.0)')\n",
        "ax.set_xlabel('GHB Conductance Multiplier', fontsize=12)\n",
        "ax.set_ylabel('Net GHB Flow [m³/d]', fontsize=12)\n",
        "ax.set_title('Sensitivity of GHB Flow to Conductance', fontsize=14)\n",
        "ax.set_xscale('linear')  # Log scale on x-axis to match the distribution\n",
        "ax.grid(True, alpha=0.3, which='both')\n",
        "ax.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pKBi_0HnvEXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}